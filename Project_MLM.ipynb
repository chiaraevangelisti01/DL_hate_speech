{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd815b23-48cf-4e2a-bb74-5ce66314c543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:44, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.672476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.523716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.684331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.732661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.231956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_mlm_model/tokenizer_config.json',\n",
       " './fine_tuned_mlm_model/special_tokens_map.json',\n",
       " './fine_tuned_mlm_model/vocab.txt',\n",
       " './fine_tuned_mlm_model/added_tokens.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import torch\n",
    "import random \n",
    "\n",
    "# Charger le tokenizer et le mod√®le BERT pr√©-entra√Æn√© pour MLM\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Charger votre dataset\n",
    "# Remplacez par le chargement de votre propre dataset\n",
    "dataset = load_dataset('SALT-NLP/silent_signals_detection')\n",
    "\n",
    "# Tokeniser le dataset\n",
    "def clean_text(text):\n",
    "    # Convertir en minuscules\n",
    "    text = text.lower()\n",
    "    # Supprimer les caract√®res sp√©ciaux et les chiffres\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # ? \n",
    "    # Supprimer les espaces superflus\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "    \n",
    "def tokenize_function(examples):\n",
    "    # Appliquer le nettoyage du texte\n",
    "    examples['example'] = [clean_text(text) for text in examples['example']]\n",
    "    # Tokeniser le texte nettoy√©\n",
    "    return tokenizer(examples['example'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "def transform_label(example):\n",
    "    example['label'] = 1 if example['label'] == \"coded\" else 0\n",
    "    return example\n",
    "\n",
    "\n",
    "def mask_tokens(examples, mlm_probability=0.15):\n",
    "    examples['example'] = [clean_text(text) for text in examples['example']]\n",
    "\n",
    "    tokenized_inputs = tokenizer(examples['example'], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    for i, tokens in enumerate(tokenized_inputs['input_ids']):\n",
    "        # Masquer les \"dog whistles\"\n",
    "        dw = examples['dog_whistle'][i]\n",
    "        dw_ids = tokenizer(dw, add_special_tokens=False)['input_ids']\n",
    "        for start_idx in range(len(tokens) - len(dw_ids) + 1):\n",
    "            if tokens[start_idx:start_idx+len(dw_ids)].equal(torch.tensor(dw_ids)):\n",
    "                for idx in range(start_idx, start_idx+len(dw_ids)):\n",
    "                    tokens[idx] = tokenizer.mask_token_id\n",
    "\n",
    "        # Masquer des tokens al√©atoires\n",
    "        for idx in range(len(tokens)):\n",
    "            if random.random() < mlm_probability and tokens[idx] not in [tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id]:\n",
    "                tokens[idx] = tokenizer.mask_token_id\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "dog_whistle_list = list(set(dataset[\"train\"][\"dog_whistle\"]))\n",
    "\n",
    "\n",
    "#tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = dataset.map(lambda examples: mask_tokens(examples), batched=True)\n",
    "                                 \n",
    "if 'label' in tokenized_datasets[\"train\"].column_names:\n",
    "    tokenized_datasets = tokenized_datasets.remove_columns('label')\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "\n",
    "# Pr√©parer les donn√©es pour l'entra√Ænement\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(50))  # Exemple avec 1000 √©chantillons\n",
    "eval_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(10))  # Exemple avec 500 √©chantillons\n",
    "\n",
    "# Configurer les arguments d'entra√Ænement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialiser le Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator = data_collator,\n",
    ")\n",
    "\n",
    "# Lancer l'entra√Ænement\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./fine_tuned_mlm_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_mlm_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d04f8082-da7e-4931-9990-bed6356171c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskedLMOutput(loss=None, logits=tensor([[[ -6.8968,  -6.7949,  -6.8031,  ...,  -6.2194,  -6.1713,  -4.3155],\n",
      "         [-12.8394, -12.3245, -12.4436,  ..., -10.3723, -10.9423, -10.4785],\n",
      "         [-12.4126, -11.7922, -12.3494,  ...,  -9.5282, -12.4392, -10.6429],\n",
      "         ...,\n",
      "         [-10.0694, -10.0425,  -9.7964,  ...,  -9.1855,  -8.8852, -11.6839],\n",
      "         [-15.9588, -15.5124, -15.5810,  ..., -11.4256, -12.2536, -13.1669],\n",
      "         [-13.0300, -13.0519, -13.0548,  ...,  -9.5997,  -9.9166, -10.5098]]],\n",
      "       device='cuda:0'), hidden_states=None, attentions=None)\n",
      "Phrase d'origine : I have bad news for all the [MASK] and enablers‚ÄîBy the time I am done reporting on these ‚Äúall ages‚Äù and ‚Äúfamily friendly‚Äù drag shows in TX, they‚Äôre going to be illegal, and there is nothing you can do about it.\n",
      "Pr√©dictions : ['promoters']\n",
      "[{'score': 0.25800225138664246, 'token': 26512, 'token_str': 'promoters', 'sequence': 'i have bad news for all the promoters and enablers ‚Äî by the time i am done reporting on these ‚Äú all ages ‚Äù and ‚Äú family friendly ‚Äù drag shows in tx, they ‚Äô re going to be illegal, and there is nothing you can do about it.'}, {'score': 0.16032348573207855, 'token': 18829, 'token_str': 'organizers', 'sequence': 'i have bad news for all the organizers and enablers ‚Äî by the time i am done reporting on these ‚Äú all ages ‚Äù and ‚Äú family friendly ‚Äù drag shows in tx, they ‚Äô re going to be illegal, and there is nothing you can do about it.'}, {'score': 0.09220732748508453, 'token': 13162, 'token_str': 'sponsors', 'sequence': 'i have bad news for all the sponsors and enablers ‚Äî by the time i am done reporting on these ‚Äú all ages ‚Äù and ‚Äú family friendly ‚Äù drag shows in tx, they ‚Äô re going to be illegal, and there is nothing you can do about it.'}, {'score': 0.05896756425499916, 'token': 4599, 'token_str': 'fans', 'sequence': 'i have bad news for all the fans and enablers ‚Äî by the time i am done reporting on these ‚Äú all ages ‚Äù and ‚Äú family friendly ‚Äù drag shows in tx, they ‚Äô re going to be illegal, and there is nothing you can do about it.'}, {'score': 0.029322896152734756, 'token': 5608, 'token_str': 'owners', 'sequence': 'i have bad news for all the owners and enablers ‚Äî by the time i am done reporting on these ‚Äú all ages ‚Äù and ‚Äú family friendly ‚Äù drag shows in tx, they ‚Äô re going to be illegal, and there is nothing you can do about it.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, pipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "# Exemple de phrase\n",
    "phrase = 'I have bad news for all the [MASK] and enablers‚ÄîBy the time I am done reporting on these ‚Äúall ages‚Äù and ‚Äúfamily friendly‚Äù drag shows in TX, they‚Äôre going to be illegal, and there is nothing you can do about it.'\n",
    "\n",
    "# Tokeniser la phrase\n",
    "inputs = tokenizer(phrase, return_tensors=\"pt\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "inputs = inputs.to(device)\n",
    "# Obtenir les pr√©dictions du mod√®le\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print(outputs)\n",
    "# Obtenir les pr√©dictions pour le token masqu√©\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "predicted_token_ids = torch.argmax(outputs.logits[0, mask_token_index], dim=1)\n",
    "predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids)\n",
    "\n",
    "# Afficher les pr√©dictions\n",
    "print(\"Phrase d'origine :\", phrase)\n",
    "print(\"Pr√©dictions :\", predicted_tokens)\n",
    "\n",
    "# Utiliser la pipeline pour une utilisation plus simple\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "print(fill_mask(phrase))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7845c392-addf-403d-a652-b0b7b2f17728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['norm17',\n",
       " '#Milk',\n",
       " 'norm24',\n",
       " 'gibsmedat',\n",
       " 'magapede',\n",
       " '#GenderWooWoo',\n",
       " 'norm23',\n",
       " 'norm20',\n",
       " 'groomer',\n",
       " 'norm8',\n",
       " 'norm14',\n",
       " 'soy',\n",
       " 'coincidence',\n",
       " 'norm4',\n",
       " 'Zioworld',\n",
       " '109',\n",
       " 'fatherless child',\n",
       " 'Durden',\n",
       " 'Rothschilds',\n",
       " 'norm9',\n",
       " 'alarmist',\n",
       " 'jogger',\n",
       " 'norm0',\n",
       " 'norm15',\n",
       " 'grooming',\n",
       " 'bop',\n",
       " 'fatherless',\n",
       " 'steroids',\n",
       " 'norm5',\n",
       " 'majority-minority',\n",
       " 'alt-right',\n",
       " 'cabal',\n",
       " 'terrorist',\n",
       " 'norm10',\n",
       " 'norm16',\n",
       " 'norm2',\n",
       " 'groomers',\n",
       " 'norm19',\n",
       " 'norm13',\n",
       " 'anointed',\n",
       " 'clownfish',\n",
       " 'hygienic',\n",
       " 'windmill',\n",
       " 'nibba',\n",
       " 'octopus',\n",
       " 'norm22',\n",
       " 'based',\n",
       " 'joggers',\n",
       " 'norm3',\n",
       " 'Google',\n",
       " 'globalism',\n",
       " 'norm1',\n",
       " 'norm7',\n",
       " 'thug',\n",
       " 'globalist',\n",
       " 'autogynephilia',\n",
       " 'genderfree',\n",
       " 'Aiden',\n",
       " 'terrorists',\n",
       " 'norm12',\n",
       " 'Reagan',\n",
       " 'echo',\n",
       " 'kosherist',\n",
       " 'Islamists',\n",
       " 'norm11',\n",
       " 'Boogaloo',\n",
       " 'Judeo-Christian',\n",
       " 'TRAs',\n",
       " 'Illuminati',\n",
       " 'Pajeet',\n",
       " 'intact',\n",
       " 'fren',\n",
       " 'norm6',\n",
       " 'Kek',\n",
       " 'YWNBAW',\n",
       " 'womyn',\n",
       " 'womanface',\n",
       " 'Khazars',\n",
       " 'COIN',\n",
       " 'XX',\n",
       " 'shoah',\n",
       " '23/16',\n",
       " 'norm18',\n",
       " 'identitarianism',\n",
       " 'norm21']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MLM + classifier ? \n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('SALT-NLP/silent_signals_detection')\n",
    "list(set(dataset[\"train\"][\"dog_whistle\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce5e191-1ddf-4e7f-ae3f-20737cdd103a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
